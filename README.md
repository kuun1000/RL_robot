ê°œìš”
---
1. ìŠ¤í„°ë”” ëª©ì (ì˜¤ë¯¼ì‹)
2. ê°•í™”í•™ìŠµ ê¸°ë³¸ ì´ë¡ (ì´ì—°ìˆ˜)
3. DRL ì„¤ëª… => DQN(ìµœì›ê°•), TD3(ì˜¤ë¯¼ì‹)

ìŠ¤í„°ë”” ëª©ì 
---
ìŠ¤í„°ë”” ê¸°ê°„ : 24.03.15 ~ 24.12.23

ë³¸ ìŠ¤í„°ë””ëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ ê°•í™”í•™ìŠµ(Deep Reinforcement Learning : DRL) ê³µë¶€ë¥¼ í†µí•´ êµ¬ì„±í•œ ìŠ¤í„°ë””ë¡œ, ê¸°ì´ˆë¶€í„° ì‹œì‘í•˜ì—¬, ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª©í‘œë¡œ í•˜ì˜€ë‹¤.

ê²°ë¡ ì ìœ¼ë¡œëŠ” ì‹¤ì œ ë¡œë´‡ ì ìš©ì—ëŠ” ì‹¤íŒ¨í•˜ì˜€ì§€ë§Œ, DRLì— ëŒ€í•œ ì´ë¡  ë° ë™ì‘ ë°©ì‹ì„ ì´í•´í•  ìˆ˜ ìˆì—ˆë‹¤.

   
TD3 (Twin Delayed Deep Deterministic Policy Gradient) ì´ë¡  ë° êµ¬ì¡° ì„¤ëª…
---
ë¡œë´‡ íŒ” Pick and Place êµ¬í˜„ì„ ìœ„í•´ TD3 (Twin Delayed Deep Deterministic Policy Gradient) ë°©ë²•ì„ ì‚¬ìš©í•˜ë ¤ê³  í•˜ì˜€ìœ¼ë‚˜, ìµœì¢…ì ìœ¼ë¡œëŠ” ì‹¤íŒ¨í•˜ì˜€ë‹¤.

í•˜ì§€ë§Œ ê³µë¶€í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì´ë¡  ì„¤ëª…ì„ ì¶”ê°€í•˜ì˜€ìœ¼ë©°, ì°¸ê³ í•œ ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ëŠ” "ppt" í´ë”ì— ì •ë¦¬ë˜ì–´ ìˆë‹¤.

TD3ëŠ” ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë¡œ, **DDPG (Deep Deterministic Policy Gradient)**ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ì´ë‹¤. 

ì£¼ë¡œ ì—°ì†ì ì¸ ì•¡ì…˜ ê³µê°„ì—ì„œ ì‘ë™í•˜ë©°, ì •ì±…ì˜ ì•ˆì •ì„±ê³¼ í•™ìŠµ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ì ì„ ë‘”ë‹¤. 

TD3ëŠ” ë‘ ê°œì˜ ì£¼ìš” ë„¤íŠ¸ì›Œí¬ì¸ Actor Networkì™€ Critic Networkë¥¼ ì‚¬ìš©í•˜ë©°, ì´ë¥¼ í†µí•´ ì •ì±…(Actor)ê³¼ ê°€ì¹˜ í•¨ìˆ˜(Critic)ë¥¼ í•™ìŠµí•œë‹¤.

âš™ï¸ 2. êµ¬ì¡°ì™€ êµ¬ì„± ìš”ì†Œ
---
A. Replay Buffer
ê²½í—˜ (s, a, r, s')ì„ ì €ì¥í•œë‹¤. ê²½í—˜(experience)ì€ agentê°€ environmentì™€ ìƒí˜¸ì‘ìš©í•œ ì •ë³´ë¥¼ ì˜ë¯¸í•˜ë©°, DRL(Deep Reinforcement Learning)ì€ ê²½í—˜ì„ training dataë¡œ í™œìš©í•œë‹¤.

ëª©ì : ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ê³  í•™ìŠµì„ ì•ˆì •í™”í•˜ê¸° ìœ„í•´ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì—ì„œ ìˆ˜ì§‘í•œ ê²½í—˜ì„ replay bufferì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•œë‹¤.

B. Critic Network
ê¸°ëŠ¥: ìƒíƒœ-ì•¡ì…˜ ìŒ (s, a)ì— ëŒ€í•œ Q-ê°’ì„ ì¶”ì •í•œë‹¤.

êµ¬ì„±: ë‘ ê°œì˜ ë…ë¦½ì ì¸ Q-í•¨ìˆ˜ critic1, critic2ì™€ ì´ë“¤ì˜ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ target1, target2ë¡œ êµ¬ì„±ëœë‹¤.

TD Error Update (Temporal Difference Error): ë‘ Q-ë„¤íŠ¸ì›Œí¬ëŠ” TD ì—ëŸ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤.

Target Q ê°’ ë¹„êµ: ë‘ Critic ë„¤íŠ¸ì›Œí¬ì—ì„œ Q ê°’ì„ ì˜ˆì¸¡í•œ í›„, ë” ì‘ì€ Q ê°’ì„ ì„ íƒí•˜ì—¬ overestimation biasë¥¼ ì¤„ì¸ë‹¤.

Critic Network í•™ìŠµ ë‹¨ê³„:

1. critic1ê³¼ critic2ëŠ” ê°ê°ì˜ Q-ê°’ì„ ì˜ˆì¸¡í•œë‹¤.

2. íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬(target1, target2)ëŠ” ëª©í‘œ Q-ê°’ì„ ê³„ì‚°í•œë‹¤. ë‘ Q-ê°’ ì¤‘ ë” ì‘ì€ ê°’ì„ ì„ íƒí•˜ì—¬ í•™ìŠµí•œë‹¤.

C. Actor Network

ê¸°ëŠ¥: ì£¼ì–´ì§„ ìƒíƒœ sì— ëŒ€í•´ ìµœì ì˜ í–‰ë™ aë¥¼ ìƒì„±í•œë‹¤.

êµ¬ì„±: actorì™€ target ë‘ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ëœë‹¤.

DPG Update (Deterministic Policy Gradient): Actor NetworkëŠ” Critic Networkì—ì„œ ì „ë‹¬ë°›ì€ Q-ê°’ì„ ìµœëŒ€í™”í•˜ë„ë¡ ì—…ë°ì´íŠ¸ëœë‹¤.

Actor Network í•™ìŠµ ë‹¨ê³„:

1. Actor NetworkëŠ” ì£¼ì–´ì§„ ìƒíƒœ sì— ëŒ€í•´ í–‰ë™ Î¼(s)ë¥¼ ìƒì„±í•œë‹¤.

2. í–‰ë™ì— ë…¸ì´ì¦ˆ Në¥¼ ì¶”ê°€í•˜ì—¬ íƒìƒ‰ì„ ìˆ˜í–‰í•œë‹¤.

3. Critic Networkë¡œë¶€í„° í”¼ë“œë°±ì„ ë°›ì•„ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•œë‹¤.

D. Environment

Actor Networkì—ì„œ ìƒì„±ëœ í–‰ë™ aëŠ” í™˜ê²½ì— ì „ë‹¬ë˜ê³ , í™˜ê²½ì€ í•´ë‹¹ í–‰ë™ì„ ë°›ì•„ë“¤ì—¬ ë‹¤ìŒ ìƒíƒœ s'ì™€ ë³´ìƒ rì„ ë°˜í™˜í•œë‹¤.

ë°˜í™˜ëœ (s, a, r, s') ì •ë³´ experienceëŠ” Replay Bufferì— ì €ì¥ëœë‹¤.

ğŸ”‘ 3. TD3ì˜ í•µì‹¬
---
Double Q-Learning

ë‘ Critic Networkë¥¼ ì‚¬ìš©í•˜ì—¬ Q-ê°’ì˜ ê³¼ëŒ€í‰ê°€ë¥¼ ë°©ì§€í•œë‹¤.
Target Policy Smoothing

Actor Networkì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ê¸‰ê²©í•œ ì •ì±… ë³€í™”ì™€ ê³¼ì í•©ì„ ë°©ì§€í•œë‹¤.
Delayed Policy Update

Critic Networkê°€ ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ í•™ìŠµë˜ëŠ” ë°˜ë©´, Actor NetworkëŠ” ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ì—…ë°ì´íŠ¸ëœë‹¤.
ì´ë¡œ ì¸í•´ Critic Networkê°€ ë” ì•ˆì •ì ì¸ Q-ê°’ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

ğŸ› ï¸ 4. TD3 ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ìˆœì„œ
---
1) ì´ˆê¸°í™” ë‹¨ê³„

1-1) Actorì™€ Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”

1-2) Replay Buffer ì´ˆê¸°í™”

2) ê²½í—˜ ìˆ˜ì§‘

2-1) Actor ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ í–‰ë™ a_t ì„ íƒ

2-2) í™˜ê²½ì— í–‰ë™ ì „ë‹¬ ë° ë³´ìƒê³¼ ë‹¤ìŒ ìƒíƒœ (r_t, s_{t+1}) ë°˜í™˜

2-3) ê²½í—˜ (s_t, a_t, r_t, s_{t+1})ì„ Replay Bufferì— ì €ì¥

3) ìƒ˜í”Œë§ ë° í•™ìŠµ

3-1) Replay Bufferì—ì„œ ë¬´ì‘ìœ„ë¡œ ê²½í—˜ ìƒ˜í”Œë§

3-2) Critic ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (TD ì—ëŸ¬ ê¸°ë°˜)

3-3) Actor ë„¤íŠ¸ì›Œí¬ëŠ” Critic ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°ë§ˆë‹¤ í•™ìŠµ

4) íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸

4-1) Critic íƒ€ê²Ÿê³¼ Actor íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” ì§€ì—°ëœ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ ìˆ˜í–‰

5) ë°˜ë³µ

5-1) ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ì •ì±…ê³¼ Q-í•¨ìˆ˜ë¥¼ ìµœì í™”

5. project ì£¼ì œ ì„¤ëª…=> Pick and Place(ìµœì›ê°•)
6. requiremnets(ìµœì›ê°•)
7. ref code process architecture(ì˜¥ìœ¤ì •)
8. ref code ì„¤ëª…(ì´ì—°ìˆ˜)
9. ì‹œí–‰ì°©ì˜¤ë“¤(image2action, camera2world coordinate, etc...)(ì˜¥ìœ¤ì •)
10. ìµœì¢… ê²°ê³¼(ì˜¤ë¯¼ì‹)
